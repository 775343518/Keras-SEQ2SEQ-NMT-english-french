{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\santh_000\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Input, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "import urllib.request\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "HIDDEN_UNITS = 256\n",
    "NUM_SAMPLES = 10000\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "EMBEDDING_SIZE = 200\n",
    "DATA_PATH = 'fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counter = Counter()\n",
    "\n",
    "GLOVE_MODEL = \"glove.6B.100d.txt\"\n",
    "fr_w2v = KeyedVectors.load_word2vec_format('frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin', binary=True)\n",
    "WEIGHT_FILE_PATH = 'eng-to-fr-glove-weights.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(DATA_PATH, 'rt', encoding='utf8').read().split('\\n')\n",
    "for line in lines[: min(NUM_SAMPLES, len(lines)-1)]:\n",
    "    target_text,input_text  = line.split('\\t')\n",
    "    input_words = [w for w in nltk.word_tokenize(input_text.lower())]\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    for char in target_text:\n",
    "        target_counter[char] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'enflammai l'allumette.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tI lit the match.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word2idx = dict()\n",
    "for idx, word in enumerate(target_counter.most_common(MAX_VOCAB_SIZE)):\n",
    "    #print(word)\n",
    "    target_word2idx[word[0]] = idx\n",
    "\n",
    "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "\n",
    "num_decoder_tokens = len(target_idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_emb = np.random.randn(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_seq_length = 0\n",
    "decoder_max_seq_length = 0\n",
    "\n",
    "input_texts_word2em = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(DATA_PATH, 'rt', encoding='utf8').read().split('\\n')\n",
    "for line in lines[: min(NUM_SAMPLES, len(lines)-1)]:\n",
    "    target_text,input_text = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_words = [w for w in nltk.word_tokenize(input_text.lower())]\n",
    "    encoder_input_wids = []\n",
    "    for w in input_words:\n",
    "        em = unknown_emb\n",
    "        if w in fr_w2v:\n",
    "            em = fr_w2v[w]\n",
    "        encoder_input_wids.append(em)\n",
    "    input_texts_word2em.append(encoder_input_wids)\n",
    "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
    "    decoder_max_seq_length = max(len(target_text), decoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = pad_sequences(input_texts_word2em, encoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data = np.zeros(shape=(NUM_SAMPLES, decoder_max_seq_length, num_decoder_tokens))\n",
    "decoder_input_data = np.zeros(shape=(NUM_SAMPLES, decoder_max_seq_length, num_decoder_tokens))\n",
    "lines = open(DATA_PATH, 'rt', encoding='utf8').read().split('\\n')\n",
    "for lineIdx, line in enumerate(lines[: min(NUM_SAMPLES, len(lines)-1)]):\n",
    "    target_text,_  = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    for idx, char in enumerate(target_text):\n",
    "        if char in target_word2idx:\n",
    "            w2idx = target_word2idx[char]\n",
    "            decoder_input_data[lineIdx, idx, w2idx] = 1\n",
    "            if idx > 0:\n",
    "                decoder_target_data[lineIdx, idx-1, w2idx] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dict()\n",
    "context['num_decoder_tokens'] = num_decoder_tokens\n",
    "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
    "context['decoder_max_seq_length'] = decoder_max_seq_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 2.0658 - val_loss: 2.2923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santh_000\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\keras\\engine\\topology.py:2368: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder_lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 1.6166 - val_loss: 1.8556\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 1.3606 - val_loss: 1.6914\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 36s 5ms/step - loss: 1.2275 - val_loss: 1.6312\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 36s 4ms/step - loss: 1.1321 - val_loss: 1.5607\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 36s 4ms/step - loss: 1.0512 - val_loss: 1.4778\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 35s 4ms/step - loss: 0.9825 - val_loss: 1.4820\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 35s 4ms/step - loss: 0.9199 - val_loss: 1.4512\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.8625 - val_loss: 1.4251\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.8104 - val_loss: 1.4510\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 35s 4ms/step - loss: 0.7605 - val_loss: 1.3936\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 36s 4ms/step - loss: 0.7140 - val_loss: 1.4205\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.6690 - val_loss: 1.5046\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6268 - val_loss: 1.4969\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5851 - val_loss: 1.4480\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5469 - val_loss: 1.4922\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.5114 - val_loss: 1.5039\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 35s 4ms/step - loss: 0.4767 - val_loss: 1.5494\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 37s 5ms/step - loss: 0.4433 - val_loss: 1.6829\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4131 - val_loss: 1.6612\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3841 - val_loss: 1.7328\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.3573 - val_loss: 1.7902\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.3312 - val_loss: 1.8179\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.3084 - val_loss: 1.8532\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.2849 - val_loss: 1.8626\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.2643 - val_loss: 1.9154\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.2464 - val_loss: 2.0176\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2282 - val_loss: 2.0340\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 37s 5ms/step - loss: 0.2130 - val_loss: 2.0532\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1981 - val_loss: 2.0795\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.1853 - val_loss: 2.1234\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1740 - val_loss: 2.1741\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 35s 4ms/step - loss: 0.1622 - val_loss: 2.1947\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.1529 - val_loss: 2.2611\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1436 - val_loss: 2.3313\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.1362 - val_loss: 2.3184\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1282 - val_loss: 2.2765\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1229 - val_loss: 2.3684\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.1176 - val_loss: 2.3797\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1110 - val_loss: 2.4349\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1065 - val_loss: 2.5066\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.1026 - val_loss: 2.4534\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.0986 - val_loss: 2.5437\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 37s 5ms/step - loss: 0.0943 - val_loss: 2.5231\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.0919 - val_loss: 2.5388\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.0889 - val_loss: 2.5941\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.0857 - val_loss: 2.6099\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.0836 - val_loss: 2.6737\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 35s 4ms/step - loss: 0.0820 - val_loss: 2.5779\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.0793 - val_loss: 2.6446\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, EMBEDDING_SIZE), name='encoder_inputs')\n",
    "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
    "                                                                 initial_state=encoder_states)\n",
    "decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=WEIGHT_FILE_PATH, save_best_only=True)\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "          verbose=1, validation_split=0.2, callbacks=[checkpoint])\n",
    "\n",
    "model.save_weights(WEIGHT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "\n",
    "decoder_state_input_h = Input(shape=(HIDDEN_UNITS,))\n",
    "decoder_state_input_c = Input(shape=(HIDDEN_UNITS,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_lstm(decoder_inputs, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_inputs] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = context['encoder_max_seq_length']\n",
    "max_decoder_seq_length = context['decoder_max_seq_length']\n",
    "num_decoder_tokens = context['num_decoder_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_lang(input_text):\n",
    "        input_seq = []\n",
    "        input_wids = []\n",
    "        for word in nltk.word_tokenize(input_text.lower()):\n",
    "            emb = unknown_emb\n",
    "            if word in fr_w2v:\n",
    "                emb = fr_w2v[word]\n",
    "            input_wids.append(emb)\n",
    "        input_seq.append(input_wids)\n",
    "        input_seq = pad_sequences(input_seq, max_encoder_seq_length)\n",
    "        states_value = encoder_model_inf.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1,num_decoder_tokens))\n",
    "        target_seq[0, 0, target_word2idx['\\t']] = 1\n",
    "        target_text = ''\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            output_tokens, h, c = decoder_model_inf.predict([target_seq] + states_value)\n",
    "\n",
    "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "            sample_word = target_idx2word[sample_token_idx]\n",
    "            target_text += sample_word\n",
    "\n",
    "            if sample_word == '\\n' or len(target_text) >= max_decoder_seq_length:\n",
    "                terminated = True\n",
    "\n",
    "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "            target_seq[0, 0, sample_token_idx] = 1\n",
    "\n",
    "            states_value = [h, c]\n",
    "        return target_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is sick.\n"
     ]
    }
   ],
   "source": [
    "print(translate_lang('Il est malade.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfp3.6]",
   "language": "python",
   "name": "conda-env-tfp3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
